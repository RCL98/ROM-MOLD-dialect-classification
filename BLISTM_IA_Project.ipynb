{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BLISTM_IA_Project.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"zxM6GWAqyj8f","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from collections import Counter\n","import string\n","import  random\n","from spacy.lang.ro import Romanian\n","from spacy.lang.ro.stop_words import STOP_WORDS\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim \n","from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchtext import data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"chgImZts9aQf","colab_type":"code","colab":{}},"source":["spacy_ro = Romanian()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1iVgq_LMvTi","colab_type":"code","outputId":"40d61a12-fbfe-4469-9bc6-de734e85043b","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1586170646379,"user_tz":-180,"elapsed":1644,"user":{"displayName":"CRISTIAN-LAURENTIU RANETE","photoUrl":"","userId":"00557210372995011208"}}},"source":["cd /content/drive/My Drive/ranete_cristian"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/My Drive/ranete_cristian'\n","/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rN4jfn3S9dYY","colab_type":"code","colab":{}},"source":["use_cuda = torch.cuda.is_available()\n","torch.manual_seed(1024)\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","torch.backends.cudnn.deterministic = True  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLn3e1QGp66S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":339},"outputId":"6ec557ef-935a-49b9-f3a9-1fa38b467095","executionInfo":{"status":"error","timestamp":1586170646392,"user_tz":-180,"elapsed":1617,"user":{"displayName":"CRISTIAN-LAURENTIU RANETE","photoUrl":"","userId":"00557210372995011208"}}},"source":["train_data = pd.read_csv('train_data.csv', sep='\\t', encoding='utf-8', lineterminator='\\n', header = 0, names=['Id', 'Text', 'Label'])\n","validation_data = pd.read_csv('valid_data.csv', sep='\\t', encoding='utf-8', lineterminator='\\n', header = 0, names=['Id', 'Text', 'Label'])"],"execution_count":7,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-c151438e4f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_data.csv'"]}]},{"cell_type":"code","metadata":{"id":"0ddsD3n9jkGh","colab_type":"code","colab":{}},"source":["def full_texts(texts):\n","  text = \" \"\n","  for it in texts:\n","    text += \" \".join(it)\n","  return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wFonLEkkMNl","colab_type":"code","colab":{}},"source":["text = full_texts(train_data['Text'].tolist())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l9SwFkcVjbPx","colab_type":"code","colab":{}},"source":["class Vocabulary:\n","    \"\"\"\n","    Helper class that maps characters to unique indices and the other way around\n","    \"\"\"\n","    def __init__(self, text: str):\n","        # PAD is a special character for padding shorter sequences \n","        # in a mini-batch\n","        # create a set out of all characters\n","        characters_set = set([\"0\"]) \n","        characters_set.update(text)\n","        \n","        #create a dictionary for characters\n","        self.char_to_idx = {char:idx for (idx, char) \n","                            in enumerate(characters_set)}\n","        self.idx_to_char = {idx:char for (idx, char) \n","                            in enumerate(characters_set)}\n","   \n","    def size(self):\n","        return len(self.char_to_idx)\n","      \n","    def __str__(self):\n","        return str(self.char_to_idx)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9thlSu5kcTb","colab_type":"code","colab":{}},"source":["vocab = Vocabulary(text)\n","print(\"Vocabulary size: \", vocab.size())\n","print(\"Vocabulary: \\n\", vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CBJbAqfiSGv","colab_type":"code","colab":{}},"source":["def text_to_tensor(text: str, vocab: Vocabulary) -> torch.LongTensor:\n","    \"\"\"\n","    Convert a string to a Tensor with corresponding character indices\n","    e.g. \"We have\" -> [48, 13,  2, 66, 56, 31, 13 \n","    \"\"\"\n","    text_indices = [vocab.char_to_idx[c] for c in text]\n","  \n","    return torch.tensor(text_indices)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-n17Bx0ReBRc","colab_type":"code","colab":{}},"source":["# function that prepers bacthes \n","def my_collate(batch):\n","  sizes = []\n","  for item in batch:\n","    sizes.append(torch.tensor(len(item[0])))\n","  sizes = torch.stack(sizes, dim = 0).long()\n","  max_size = torch.max(sizes, dim = 0)[0]\n","  new_data = []\n","  for item in batch:\n","    new_data.append(F.pad(input=item[0], pad=(0, max_size - item[0].shape[0]), mode='constant', value=vocab.char_to_idx['0']))\n","  data = torch.stack(new_data, dim = 0)\n","  target = torch.stack([torch.tensor(item[1]) for item in batch], dim = 0)\n","  return [data, sizes, target]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SnzscKN3luU2","colab_type":"code","colab":{}},"source":["class TextsDataset(Dataset):\n","    def __init__(self, texts, labels=None, vocab = None, max_length = 1004):\n","        self.X = texts\n","        self.y = labels\n","        self.vocab = vocab\n","        self.max_len = max_length\n","         \n","    def __len__(self):\n","        return (len(self.X))\n","    \n","    def __getitem__(self, i):\n","        data = self.X[i]\n","        data = text_to_tensor(data, self.vocab)\n","        if self.y is not None:\n","            y = self.y[i]\n","            return (data, y)\n","        else:\n","            return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mpxPz5npaem","colab_type":"code","colab":{}},"source":["training_dataset = TextsDataset(train_data['Text'].tolist(), train_data[\"Label\"].tolist(),vocab, 1004)\n","validing_dataset = TextsDataset(validation_data[\"Text\"].tolist(), validation_data[\"Label\"].tolist(),vocab, 1004)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jtRWzDiwhQu","colab_type":"code","colab":{}},"source":["batch_size = 128"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EzhzpGVqo-_","colab_type":"code","colab":{}},"source":["trainloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=my_collate)\n","validloader = DataLoader(validing_dataset, batch_size=batch_size, shuffle=True, drop_last = True, collate_fn=my_collate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHw8VDAD_bay","colab_type":"code","colab":{}},"source":["class BiLSTM(nn.Module):\n","    \n","    def __init__(self, vocab_size, output_size, embed_size, hidden_nodes, n_layers, drop_prob=0.5):\n","        super(BiLSTM, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_nodes, bidirectional=True, batch_first=True, num_layers = n_layers)\n","        self.linear = nn.Linear(hidden_nodes*4 , 64)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(drop_prob)\n","        self.out = nn.Linear(64, output_size)\n","\n","\n","    def forward(self, x, text_sizes):\n","        h_embedding = self.embedding(x)\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(h_embedding, text_sizes.flatten(), batch_first=True, enforce_sorted=False)\n","        h_lstm, _ = self.lstm(packed_embedded)\n","        h_lstm = nn.utils.rnn.pad_packed_sequence(h_lstm, True)[0]\n","        avg_pool = torch.mean(h_lstm, 1)\n","        max_pool, _ = torch.max(h_lstm, 1)\n","        conc = torch.cat(( avg_pool, max_pool), 1)\n","        conc = self.relu(self.linear(conc))\n","        conc = self.dropout(conc)\n","        out = torch.sigmoid(self.out(conc))\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DfrrGOXAeif","colab_type":"code","colab":{}},"source":["size_of_vocab = vocab.size()\n","embedding_dim = 100\n","num_hidden_nodes = 64\n","num_output_nodes = 1\n","num_layers = 2\n","dropout = 0.2\n","\n","#instantiate the model\n","model_r = BiLSTM(size_of_vocab, num_output_nodes, embedding_dim, num_hidden_nodes, num_layers, dropout)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gSXT8u52A4SD","colab_type":"code","colab":{}},"source":["#architecture\n","print(model_r)\n","\n","#No. of trianable parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model_r.parameters() if p.requires_grad)\n","    \n","print(f'The model has {count_parameters(model_r):,} trainable parameters')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHpaoKSpBNub","colab_type":"code","colab":{}},"source":["#define optimizer and loss\n","optimizer = optim.Adam(model_r.parameters(), lr = 0.005)\n","criterion = nn.BCELoss()\n","\n","#define metric\n","def binary_accuracy(preds, y):\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(preds)\n","    \n","    correct = (rounded_preds == y).float() \n","    acc = correct.sum() / len(correct)\n","    return acc\n","    \n","#push to cuda if available\n","model_r = model_r.to(device)\n","criterion = criterion.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZDwmqRlBbYS","colab_type":"code","colab":{}},"source":["def train(model, train_iterator, optimizer, criterion):\n","    \n","    #initialize every epoch \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    clip = 3\n","    \n","    #set the model in training phase\n","    model.train()  \n","    for inputs, text_lengths, labels in train_iterator:\n","        #print(it)\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        #resets the gradients after every batch\n","        optimizer.zero_grad()   \n","        \n","        #retrieve text and no. of words\n","        #print(\"retrive\")\n","        output = model(inputs, text_lengths) \n","        \n","        #compute the loss\n","        #print(\"loss\")\n","        loss = criterion(output.squeeze(), labels.float())  \n","        \n","        #compute the binary accuracy\n","        #print(\"acc\")\n","        acc = binary_accuracy(output.squeeze(), labels)   \n","        \n","        #print(\"back\")\n","        #backpropage the loss and compute the gradients\n","        loss.backward() \n","        nn.utils.clip_grad_norm_(model.parameters(), clip)      \n","        \n","        #print(\"optim\")\n","        #update the weights\n","        optimizer.step()      \n","        \n","        #loss and accuracy\n","        epoch_loss += loss.item()  \n","        epoch_acc += acc.item()      \n","    return epoch_loss / len(train_iterator), epoch_acc / len(train_iterator)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Nviwv9XB2Vr","colab_type":"code","colab":{}},"source":["def evaluate(model, eval_iterator, criterion):\n","    \n","    #initialize every epoch\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    clip = 5\n","\n","    #deactivating dropout layers\n","    model.eval()\n","    \n","    #deactivates autograd\n","    with torch.no_grad():\n","    \n","        for inputs, text_lengths, labels in eval_iterator:\n","\n","            #retrieve text and no. of words\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            \n","            #convert to 1d tensor\n","            output = model(inputs, text_lengths)\n","            \n","            #compute loss and accuracy\n","            test_loss = criterion(output.squeeze(), labels.float())\n","            acc = binary_accuracy(output.squeeze(), labels) \n","            \n","            #keep track of loss and accuracy\n","            epoch_loss += test_loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(eval_iterator), epoch_acc / len(eval_iterator)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"54Th5uzGB94j","colab_type":"code","colab":{}},"source":["N_EPOCHS = 10\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","     \n","    #train the model\n","    train_loss, train_acc = train(model_r, trainloader, optimizer, criterion)\n","    \n","    print(\"eval\")\n","    #evaluate the model\n","    valid_loss, valid_acc = evaluate(model_r, validloader, criterion)\n","     \n","   #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model_r.state_dict(), 'saved_weights_1.pt')\n","    \n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iJL1TjlMe8M5","colab_type":"code","colab":{}},"source":["model_r.load_state_dict(torch.load(\"saved_weights_1.pt\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FetelSjTOryl","colab_type":"code","colab":{}},"source":["target_loss, target_acc = evaluate(model_r, target_iterator, criterion)"],"execution_count":0,"outputs":[]}]}